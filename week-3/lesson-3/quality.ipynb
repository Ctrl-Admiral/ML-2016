{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "data = pd.read_csv(open('classification.csv'))\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tp = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "tn = 0\n",
    "for _, rdata in data.iterrows():\n",
    "    yi = int(rdata['true'])\n",
    "    ai = int(rdata['pred'])\n",
    "    tp += yi*ai\n",
    "    fp += (1-yi)*ai\n",
    "    fn += yi*(1-ai)\n",
    "    tn += (1-yi)*(1-ai)\n",
    "    \n",
    "print(\"TP = %d\" % tp)\n",
    "print(\"FP = %d\" % fp)\n",
    "print(\"FN = %d\" % fn)\n",
    "print(\"TN = %d\" % tn)\n",
    "\n",
    "with open(\"quality.1.txt\", \"w\") as file:\n",
    "    file.write(\"%d %d %d %d\" % (tp,fp,fn,tn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "accuracy  = accuracy_score(data['true'], data['pred'])\n",
    "precision = precision_score(data['true'], data['pred'])\n",
    "recall    = recall_score(data['true'], data['pred'])\n",
    "f1        = f1_score(data['true'], data['pred'])\n",
    "\n",
    "print(\"Accuracy = %.2f\" % accuracy)\n",
    "print(\"Precision = %.2f\" % precision)\n",
    "print(\"Recall = %.2f\" % recall)\n",
    "print(\"F1 = %.2f\" % f1)\n",
    "\n",
    "with open(\"quality.2.txt\", \"w\") as file:\n",
    "    file.write(\"%.2f %.2f %.2f %.2f\" % (accuracy,precision,recall,f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "scores = pd.read_csv(open('scores.csv'))\n",
    "print(scores.head())\n",
    "\n",
    "roc_logreg = roc_auc_score(scores['true'], scores['score_logreg'])\n",
    "roc_svm    = roc_auc_score(scores['true'], scores['score_svm'])\n",
    "roc_knn    = roc_auc_score(scores['true'], scores['score_knn'])\n",
    "roc_tree   = roc_auc_score(scores['true'], scores['score_tree'])\n",
    "print(\"roc_logreg = %f\" % roc_logreg)\n",
    "print(\"roc_svm = %f\"    % roc_svm)\n",
    "print(\"roc_knn = %f\"    % roc_knn)\n",
    "print(\"roc_tree = %f\"   % roc_tree)\n",
    "\n",
    "with open(\"quality.3.txt\", \"w\") as file:\n",
    "    file.write(\"score_logreg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "for v in ('score_logreg', 'score_svm', 'score_knn', 'score_tree'):\n",
    "    (precision, recall, thresholds) = precision_recall_curve(scores['true'], scores[v])\n",
    "    max_p = 0\n",
    "    for (p,r) in zip(precision,recall):\n",
    "        if (r > 0.7):\n",
    "            max_p = max(max_p, p)\n",
    "    print(\"%s max precision = %f\" % (v,max_p))\n",
    "\n",
    "with open(\"quality.4.txt\", \"w\") as file:\n",
    "    file.write(\"score_tree\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}