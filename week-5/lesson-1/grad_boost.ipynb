{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузите выборку из файла gbm-data.csv с помощью pandas и преобразуйте ее в массив numpy (параметр values у датафрейма). В первой колонке файла с данными записано, была или нет реакция. Все остальные колонки (d1 - d1776) содержат различные характеристики молекулы, такие как размер, форма и т.д. Разбейте выборку на обучающую и тестовую, используя функцию train_test_split с параметрами test_size = 0.8 и random_state = 241."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "data = pd.read_csv(open('gbm-data.csv'))\n",
    "print(data.head())\n",
    "X = data.values[:,1:]\n",
    "y = data.values[:,0]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=241)\n",
    "#print(X_train)\n",
    "#print(y_train)\n",
    "#print(X_test)\n",
    "#print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучите GradientBoostingClassifier с параметрами n_estimators=250, verbose=True, random_state=241 и для каждого значения learning_rate из списка [1, 0.5, 0.3, 0.2, 0.1] проделайте следующее:\n",
    "- Используйте метод staged_decision_function для предсказания качества на обучающей и тестовой выборке на каждой итерации.\n",
    "- Преобразуйте полученное предсказание с помощью сигмоидной функции по формуле 1 / (1 + e^{−y_pred}), где y_pred — предсказаное значение.\n",
    "- Вычислите и постройте график значений log-loss (которую можно посчитать с помощью функции sklearn.metrics.log_loss) на обучающей и тестовой выборках, а также найдите минимальное значение метрики и номер итерации, на которой оно достигается.\n",
    "\n",
    "Как можно охарактеризовать график качества на тестовой выборке, начиная с некоторой итерации: переобучение (overfitting) или недообучение (underfitting)? В ответе укажите одно из слов overfitting либо underfitting.\n",
    "\n",
    "Приведите минимальное значение log-loss и номер итерации, на котором оно достигается, при learning_rate = 0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "import math\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "with open(\"grad_boost.1.txt\", \"w\") as file:\n",
    "    file.write(\"overfitting\")\n",
    "\n",
    "for rate in (1.0, 0.5, 0.3, 0.2, 0.1):\n",
    "    clf = GradientBoostingClassifier(learning_rate=rate, n_estimators=250, verbose=True, random_state=241)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    log_loss_test = np.empty(len(clf.estimators_))\n",
    "    log_loss_train = np.empty(len(clf.estimators_))\n",
    "    \n",
    "    for i, pred in enumerate(clf.staged_decision_function(X_test)):\n",
    "        log_loss_test[i] = log_loss(y_test, 1.0 / (1.0 + np.exp(-pred)))\n",
    "        if rate == 0.2:\n",
    "            print(\"#%d %f\" % (i,log_loss_test[i]))\n",
    "    for i, pred in enumerate(clf.staged_decision_function(X_train)):\n",
    "        log_loss_train[i] = log_loss(y_train, 1.0 / (1.0 + np.exp(-pred)))\n",
    "      \n",
    "    if rate == 0.2:\n",
    "        index = np.argsort(log_loss_test)[0]\n",
    "        print(\"Best log-loss index=%d value=%.2f\" % (index,log_loss_test[index]))\n",
    "        with open(\"grad_boost.2.txt\", \"w\") as file:\n",
    "            file.write(\"%.2f %d\" % (log_loss_test[index],index))\n",
    "    \n",
    "    plt.plot(log_loss_test)\n",
    "    plt.plot(log_loss_train)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "На этих же данных обучите RandomForestClassifier с количеством деревьев, равным количеству итераций, на котором достигается наилучшее качество у градиентного бустинга из предыдущего пункта, c random_state=241 и остальными параметрами по умолчанию. Какое значение log-loss на тесте получается у этого случайного леса? (Не забывайте, что предсказания нужно получать с помощью функции predict_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = GradientBoostingClassifier(learning_rate=0.2, n_estimators=36, verbose=True, random_state=241)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict_proba(X_test)\n",
    "res = log_loss(y_test, y_pred)\n",
    "\n",
    "print(res)\n",
    "with open(\"grad_boost.3.txt\", \"w\") as file:\n",
    "    file.write(\"%.2f\" % res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}